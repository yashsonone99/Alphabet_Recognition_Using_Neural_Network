# Alphabet_Recognition_Using_Neural_Network
â€œDeep Learning-based Alphabet Recognition using TensorFlow &amp; Keras building, training, and evaluating a Neural Network for handwritten alphabet classification.â€ ğŸ”¤ğŸ¤–
# ğŸ”¤ Alphabet Recognition using Neural Network (TensorFlow & Keras)

> â€œA deep learning project that recognizes handwritten alphabets using a custom neural network built with TensorFlow and Keras.â€

---

## ğŸ’¡ Project Overview  
This project applies **Deep Learning** techniques to classify handwritten alphabets.  
Using the **Alphabets dataset**, a neural network model is built, trained, and evaluated for accurate character recognition.  
The workflow covers **data preprocessing**, **feature scaling**, **model architecture design**, and **performance visualization**.

---

## ğŸ¯ Objectives  
- Load and explore the **Alphabets dataset**  
- Preprocess and normalize input data  
- Design a **fully connected neural network** (ANN) using TensorFlow/Keras  
- Train and validate the model on unseen data  
- Evaluate model performance with metrics and visualizations  

---

## âš™ï¸ Technologies Used  
| Category | Tools / Libraries |
|-----------|------------------|
| Language | Python |
| Framework | TensorFlow, Keras |
| Libraries | Pandas, NumPy, Seaborn, Matplotlib, scikit-learn |
| Platform | Google Colab / Jupyter Notebook |

---

## ğŸ“‚ Files Included  

| File | Description |
|------|-------------|
| `Neural_Network_Alphabet.ipynb` | Main notebook with code implementation |
| `Alphabets_data.csv` | Dataset containing alphabet features |
| `README.md` | Project documentation file |

---

## ğŸ§ª Key Steps  

### 1ï¸âƒ£ Data Loading & Inspection  
- Loaded **Alphabets_data.csv** dataset  
- Checked data shape, missing values, and column details  

### 2ï¸âƒ£ Data Preprocessing  
- Converted labels into numeric form using **LabelEncoder**  
- Scaled numerical features using **StandardScaler**  
- Split data into **training (80%)** and **testing (20%)** sets  

### 3ï¸âƒ£ Model Building  
- Constructed a **Sequential model** with:
  - Input layer matching feature dimensions  
  - Two hidden layers with **ReLU activation**  
  - Output layer with **Softmax** activation  
- Compiled the model using:
  ```python
  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
